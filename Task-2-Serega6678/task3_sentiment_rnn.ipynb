{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2.3: Text classification via RNN (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will perform sentiment analysis of the IMDBs reviews by using RNN. An additional goal is to learn high abstactions of the **torchtext** module that consists of data processing utilities and popular datasets for natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchtext import datasets\n",
    "\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, lower=True)\n",
    "LABEL = LabelField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, tst = datasets.IMDB.splits(TEXT, LABEL)\n",
    "trn, vld = train.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.27 s, sys: 43.9 ms, total: 2.32 s\n",
      "Wall time: 2.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEXT.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocab.freqs is a collections.Counter object, so we can take a look at the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 225446),\n",
       " ('a', 111452),\n",
       " ('and', 110728),\n",
       " ('of', 101333),\n",
       " ('to', 93854),\n",
       " ('is', 72717),\n",
       " ('in', 63238),\n",
       " ('i', 49281),\n",
       " ('this', 48798),\n",
       " ('that', 46346)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Iterator (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we'll be using a special kind of Iterator, called the **BucketIterator**. When we pass data into a neural network, we want the data to be padded to be the same length so that we can process them in batch:\n",
    "\n",
    "e.g.\n",
    "\\[ \n",
    "\\[3, 15, 2, 7\\],\n",
    "\\[4, 1\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] -> \\[ \n",
    "\\[3, 15, 2, 7, **0**\\],\n",
    "\\[4, 1, **0**, **0**, **0**\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] \n",
    "\n",
    "If the sequences differ greatly in length, the padding will consume a lot of wasteful memory and time. The BucketIterator groups sequences of similar lengths together for each batch to minimize padding.\n",
    "\n",
    "Complete the definition of the **BucketIterator** object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "        (trn, vld, tst),\n",
    "        batch_sizes=(64, 64, 64),\n",
    "        sort=False,\n",
    "        sort_key=lambda x: len(x),\n",
    "        sort_within_batch=False,\n",
    "        device='cpu',\n",
    "        repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the output of the BucketIterator looks like. Do not be suprised **batch_first=True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    9,  9088,    48,  ...,    57,   146,     8],\n",
       "        [   63, 16788,     7,  ...,   735,    61,    10],\n",
       "        [  264,     2,    10,  ...,    14,   772,   493],\n",
       "        ...,\n",
       "        [    1,     1,     1,  ...,     1,     1,     1],\n",
       "        [    1,     1,     1,  ...,     1,     1,     1],\n",
       "        [    1,     1,     1,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__()); batch.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch has all the fields we passed to the Dataset as attributes. The batch data can be accessed through the attribute with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'fields', 'input_fields', 'target_fields', 'text', 'label'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([989, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RNN-based text classification model (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start simple first. Implement the model according to the shema below.  \n",
    "![alt text](https://miro.medium.com/max/1396/1*v-tLYQCsni550A-hznS0mw.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBaseline(nn.Module):\n",
    "    def __init__(self, hidden_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_voc = len(TEXT.vocab)\n",
    "        self.num_layers = 1\n",
    "        \n",
    "        self.emb = nn.Embedding(self.emb_voc, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, num_layers=self.num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, 2)\n",
    "            \n",
    "    def forward(self, seq):\n",
    "        seq = self.emb(seq)\n",
    "        starting_hidden = self.init_hidden(seq.shape)\n",
    "        preds, hidden = self.rnn(seq, starting_hidden)\n",
    "        preds = self.fc(hidden)\n",
    "        return preds.squeeze(), hidden\n",
    "    \n",
    "    def init_hidden(self, shape):\n",
    "        return torch.zeros((self.num_layers, shape[1], self.hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz = 50\n",
    "nh = 50\n",
    "model = RNNBaseline(nh, emb_dim=em_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNBaseline(\n",
       "  (emb): Embedding(202270, 50)\n",
       "  (rnn): GRU(50, 50)\n",
       "  (fc): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimization and the loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just a sample model\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 finished\n",
      "Iteration 100 finished\n",
      "Iteration 200 finished\n",
      "Epoch: 0, Training Loss: 0.6942323518060419, Validation Loss: 0.6958997446601674\n",
      "Iteration 300 finished\n",
      "Iteration 400 finished\n",
      "Iteration 500 finished\n",
      "Epoch: 1, Training Loss: 0.6942912655590224, Validation Loss: 0.6934277097047385\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() \n",
    "    for idx, batch in enumerate(train_iter): \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "\n",
    "        opt.zero_grad()\n",
    "        preds, _ = model(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        it = idx + epoch*len(train_iter)\n",
    "        if it % 100 == 0:\n",
    "            print('Iteration {} finished'.format(it))\n",
    "            \n",
    "    epoch_loss = running_loss / len(train_iter)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in val_iter:\n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        \n",
    "        preds, _ = model(x) \n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss /= len(val_iter)\n",
    "    val_losses.append(val_loss)\n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate performance of the trained model (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "accuracy = 0\n",
    "true_positive = 0\n",
    "false_negative = 0\n",
    "false_positive = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for batch in test_iter:\n",
    "    x = batch.text\n",
    "    y = batch.label\n",
    "    \n",
    "    preds, _= model(x)\n",
    "    preds = preds.argmax(dim=-1)\n",
    "    \n",
    "    accuracy += (preds == y).sum()\n",
    "    true_positive += ((preds == y) * y * preds).sum()\n",
    "    false_positive += ((preds != y) * (1 - y) * preds).sum()\n",
    "    false_negative += ((preds != y) * y * (1 - preds)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy.item()\n",
    "true_positive = true_positive.item()\n",
    "false_positive = false_positive.item()\n",
    "false_negative = false_negative.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy / batch_size / len(test_iter) * 100\n",
    "precision = true_positive / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 49.9121%\n",
      "Precision 0.4762\n",
      "Recall 0.008\n",
      "F1 0.0157\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy {}%'.format(round(accuracy, 4)))\n",
    "print('Precision {}'.format(round(precision, 4)))\n",
    "print('Recall {}'. format(round(recall, 4)))\n",
    "print('F1 {}'.format(round(2 * precision * recall / (precision + recall), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All experiments well be held inside one training session\n",
    "\n",
    "### 1. Slanted triangular learning rates\n",
    "### 2. Adding new preceding layers after training the main model\n",
    "### 3. Adding BatchNorm & DropOut to starting layers after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "class SLTR:\n",
    "    def __init__(self, training_iterations, cut_frac=0.1, ratio=32):\n",
    "        self.training_iterations = training_iterations\n",
    "        self.cut_frac = cut_frac\n",
    "        self.ratio = ratio\n",
    "        self.cut = floor(training_iterations * cut_frac)\n",
    "    \n",
    "    def __call__(self, it):\n",
    "        if it < self.cut:\n",
    "            p = it / self.cut\n",
    "        else:\n",
    "            p = 1 - (it - self.cut) / (self.cut * (1 / self.cut_frac - 1))\n",
    "        return (1 + p * (self.ratio - 1)) / self.ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_start = RNNBaseline(nh, emb_dim=em_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNBaseline(\n",
       "  (emb): Embedding(202270, 50)\n",
       "  (rnn): GRU(50, 50)\n",
       "  (fc): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_func = SLTR(epochs * len(train_iter))\n",
    "opt = optim.SGD(model1_start.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(opt, lambda e: scheduler_func(e))\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses1_start = []\n",
    "val_losses1_start = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 finished\n",
      "Iteration 100 finished\n",
      "Iteration 200 finished\n",
      "Epoch: 0, Training Loss: 0.6938906281533903, Validation Loss: 0.6934118851766748\n",
      "Iteration 300 finished\n",
      "Iteration 400 finished\n",
      "Iteration 500 finished\n",
      "Epoch: 1, Training Loss: 0.6937460174960811, Validation Loss: 0.6933328534586954\n",
      "Iteration 600 finished\n",
      "Iteration 700 finished\n",
      "Iteration 800 finished\n",
      "Epoch: 2, Training Loss: 0.6936473589744011, Validation Loss: 0.6939846211570805\n",
      "Iteration 900 finished\n",
      "Iteration 1000 finished\n",
      "Epoch: 3, Training Loss: 0.6937013235840485, Validation Loss: 0.6937957385839042\n",
      "Iteration 1100 finished\n",
      "Iteration 1200 finished\n",
      "Iteration 1300 finished\n",
      "Epoch: 4, Training Loss: 0.6935660351801963, Validation Loss: 0.6933389614194126\n",
      "Iteration 1400 finished\n",
      "Iteration 1500 finished\n",
      "Iteration 1600 finished\n",
      "Epoch: 5, Training Loss: 0.6935032286348134, Validation Loss: 0.6941258169836917\n",
      "Iteration 1700 finished\n",
      "Iteration 1800 finished\n",
      "Iteration 1900 finished\n",
      "Epoch: 6, Training Loss: 0.693500752649168, Validation Loss: 0.6934218144012709\n",
      "Iteration 2000 finished\n",
      "Iteration 2100 finished\n",
      "Epoch: 7, Training Loss: 0.6933841681393393, Validation Loss: 0.6934211870371285\n",
      "Iteration 2200 finished\n",
      "Iteration 2300 finished\n",
      "Iteration 2400 finished\n",
      "Epoch: 8, Training Loss: 0.6933140687263795, Validation Loss: 0.6932777014829344\n",
      "Iteration 2500 finished\n",
      "Iteration 2600 finished\n",
      "Iteration 2700 finished\n",
      "Epoch: 9, Training Loss: 0.6930952159157635, Validation Loss: 0.6932722143197464\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model1_start.train() \n",
    "    for idx, batch in enumerate(train_iter): \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "\n",
    "        opt.zero_grad()\n",
    "        preds, _ = model1_start(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        it = idx + epoch*len(train_iter)\n",
    "        if it % 100 == 0:\n",
    "            print('Iteration {} finished'.format(it))\n",
    "            \n",
    "    epoch_loss = running_loss / len(train_iter)\n",
    "    epoch_losses1_start.append(epoch_loss)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model1_start.eval()\n",
    "    for batch in val_iter:\n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        \n",
    "        preds, _ = model1_start(x) \n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss /= len(val_iter)\n",
    "    val_losses1_start.append(val_loss)\n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "accuracy1_start = 0\n",
    "true_positive1_start = 0\n",
    "false_negative1_start = 0\n",
    "false_positive1_start = 0\n",
    "\n",
    "model1_start.eval()\n",
    "\n",
    "for batch in test_iter:\n",
    "    x = batch.text\n",
    "    y = batch.label\n",
    "    \n",
    "    preds, _= model1_start(x)\n",
    "    preds = preds.argmax(dim=-1)\n",
    "    \n",
    "    accuracy1_start += (preds == y).sum()\n",
    "    true_positive1_start += ((preds == y) * y * preds).sum()\n",
    "    false_positive1_start += ((preds != y) * (1 - y) * preds).sum()\n",
    "    false_negative1_start += ((preds != y) * y * (1 - preds)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy1_start = accuracy1_start.item()\n",
    "true_positive1_start = true_positive1_start.item()\n",
    "false_positive1_start = false_positive1_start.item()\n",
    "false_negative1_start = false_negative1_start.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy1_start = accuracy1_start / batch_size / len(test_iter) * 100\n",
    "precision1_start = true_positive1_start / (true_positive1_start + false_positive1_start)\n",
    "recall1_start = true_positive1_start / (true_positive1_start + false_negative1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 49.9321%\n",
      "Precision 0.4999\n",
      "Recall 0.9914\n",
      "F1 0.6646\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy {}%'.format(round(accuracy1_start, 4)))\n",
    "print('Precision {}'.format(round(precision1_start, 4)))\n",
    "print('Recall {}'. format(round(recall1_start, 4)))\n",
    "print('F1 {}'.format(round(2 * precision1_start * recall1_start / (precision1_start + recall1_start), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model1_start.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewStackModel(nn.Module):\n",
    "    def __init__(self, batch_size, input_size, fc_hidden_1, fc_hidden_2, model, p1=0.5, p2=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.fc_hidden_1 = fc_hidden_1\n",
    "        self.fc_hidden_2 = fc_hidden_2\n",
    "        self.p1 = p1\n",
    "        self.p2 = p2\n",
    "        self.model = model\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(batch_size)\n",
    "        self.rulu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p1)\n",
    "        self.fc1 = nn.Linear(input_size, fc_hidden_2)\n",
    "        self.bn2 = nn.BatchNorm1d(batch_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p2)\n",
    "        self.fc2 = nn.Linear(fc_hidden_2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, x = self.model(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.rulu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "hidden1 = 50\n",
    "hidden2 = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = NewStackModel(batch_size, nh, hidden1, hidden2, model1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewStackModel(\n",
       "  (model): RNNBaseline(\n",
       "    (emb): Embedding(202270, 50)\n",
       "    (rnn): GRU(50, 50)\n",
       "    (fc): Linear(in_features=50, out_features=2, bias=True)\n",
       "  )\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (rulu1): ReLU()\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=50, out_features=50, bias=True)\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "scheduler_func = SLTR(epochs * len(train_iter))\n",
    "opt = optim.SGD(model1.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(opt, lambda e: scheduler_func(e))\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses1 = []\n",
    "val_losses1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 finished\n",
      "Iteration 100 finished\n",
      "Iteration 200 finished\n",
      "Epoch: 0, Training Loss: 0.7334918153547022, Validation Loss: 0.6880050321756783\n",
      "Iteration 300 finished\n",
      "Iteration 400 finished\n",
      "Iteration 500 finished\n",
      "Epoch: 1, Training Loss: 0.7059466079638822, Validation Loss: 0.6879299434564882\n",
      "Iteration 600 finished\n",
      "Iteration 700 finished\n",
      "Iteration 800 finished\n",
      "Epoch: 2, Training Loss: 0.6972469056609774, Validation Loss: 0.6873857282986076\n",
      "Iteration 900 finished\n",
      "Iteration 1000 finished\n",
      "Epoch: 3, Training Loss: 0.6939943772162834, Validation Loss: 0.6884561034582429\n",
      "Iteration 1100 finished\n",
      "Iteration 1200 finished\n",
      "Iteration 1300 finished\n",
      "Epoch: 4, Training Loss: 0.6930140352597202, Validation Loss: 0.6875296320955632\n",
      "Iteration 1400 finished\n",
      "Iteration 1500 finished\n",
      "Iteration 1600 finished\n",
      "Epoch: 5, Training Loss: 0.6925615607386958, Validation Loss: 0.6880122645426605\n",
      "Iteration 1700 finished\n",
      "Iteration 1800 finished\n",
      "Iteration 1900 finished\n",
      "Epoch: 6, Training Loss: 0.6921830118572625, Validation Loss: 0.6873379935652523\n",
      "Iteration 2000 finished\n",
      "Iteration 2100 finished\n",
      "Epoch: 7, Training Loss: 0.6921411676563486, Validation Loss: 0.6875400149216087\n",
      "Iteration 2200 finished\n",
      "Iteration 2300 finished\n",
      "Iteration 2400 finished\n",
      "Epoch: 8, Training Loss: 0.6917630595012303, Validation Loss: 0.6872843251389972\n",
      "Iteration 2500 finished\n",
      "Iteration 2600 finished\n",
      "Iteration 2700 finished\n",
      "Epoch: 9, Training Loss: 0.6916800735205629, Validation Loss: 0.6880291442749864\n",
      "Iteration 2800 finished\n",
      "Iteration 2900 finished\n",
      "Iteration 3000 finished\n",
      "Epoch: 10, Training Loss: 0.6913099719660125, Validation Loss: 0.6875326865810459\n",
      "Iteration 3100 finished\n",
      "Iteration 3200 finished\n",
      "Epoch: 11, Training Loss: 0.6917335451954473, Validation Loss: 0.6877486407756805\n",
      "Iteration 3300 finished\n",
      "Iteration 3400 finished\n",
      "Iteration 3500 finished\n",
      "Epoch: 12, Training Loss: 0.6916250734433641, Validation Loss: 0.6872631836745698\n",
      "Iteration 3600 finished\n",
      "Iteration 3700 finished\n",
      "Iteration 3800 finished\n",
      "Epoch: 13, Training Loss: 0.691047324316345, Validation Loss: 0.6880455188832041\n",
      "Iteration 3900 finished\n",
      "Iteration 4000 finished\n",
      "Iteration 4100 finished\n",
      "Epoch: 14, Training Loss: 0.691420482022919, Validation Loss: 0.6872761532411737\n",
      "Iteration 4200 finished\n",
      "Iteration 4300 finished\n",
      "Epoch: 15, Training Loss: 0.6910974748813323, Validation Loss: 0.6873121211084269\n",
      "Iteration 4400 finished\n",
      "Iteration 4500 finished\n",
      "Iteration 4600 finished\n",
      "Epoch: 16, Training Loss: 0.6913677730264455, Validation Loss: 0.6874024984190019\n",
      "Iteration 4700 finished\n",
      "Iteration 4800 finished\n",
      "Iteration 4900 finished\n",
      "Epoch: 17, Training Loss: 0.691026151832873, Validation Loss: 0.6874199120675103\n",
      "Iteration 5000 finished\n",
      "Iteration 5100 finished\n",
      "Iteration 5200 finished\n",
      "Epoch: 18, Training Loss: 0.6918461479409768, Validation Loss: 0.6873217957504725\n",
      "Iteration 5300 finished\n",
      "Iteration 5400 finished\n",
      "Epoch: 19, Training Loss: 0.691188930377473, Validation Loss: 0.687288700018899\n",
      "Iteration 5500 finished\n",
      "Iteration 5600 finished\n",
      "Iteration 5700 finished\n",
      "Epoch: 20, Training Loss: 0.6911892458035128, Validation Loss: 0.6872509894734722\n",
      "Iteration 5800 finished\n",
      "Iteration 5900 finished\n",
      "Iteration 6000 finished\n",
      "Epoch: 21, Training Loss: 0.6911770181499258, Validation Loss: 0.6878637580548302\n",
      "Iteration 6100 finished\n",
      "Iteration 6200 finished\n",
      "Iteration 6300 finished\n",
      "Epoch: 22, Training Loss: 0.6908379414655866, Validation Loss: 0.6873287028175289\n",
      "Iteration 6400 finished\n",
      "Iteration 6500 finished\n",
      "Epoch: 23, Training Loss: 0.6909825773569789, Validation Loss: 0.6872651804301698\n",
      "Iteration 6600 finished\n",
      "Iteration 6700 finished\n",
      "Iteration 6800 finished\n",
      "Epoch: 24, Training Loss: 0.691094606244651, Validation Loss: 0.6872659174062438\n",
      "Iteration 6900 finished\n",
      "Iteration 7000 finished\n",
      "Iteration 7100 finished\n",
      "Epoch: 25, Training Loss: 0.6906255400963943, Validation Loss: 0.6872827905719563\n",
      "Iteration 7200 finished\n",
      "Iteration 7300 finished\n",
      "Epoch: 26, Training Loss: 0.691247708823559, Validation Loss: 0.6873091651221453\n",
      "Iteration 7400 finished\n",
      "Iteration 7500 finished\n",
      "Iteration 7600 finished\n",
      "Epoch: 27, Training Loss: 0.6910098510502029, Validation Loss: 0.6873046402203835\n",
      "Iteration 7700 finished\n",
      "Iteration 7800 finished\n",
      "Iteration 7900 finished\n",
      "Epoch: 28, Training Loss: 0.6909211230103987, Validation Loss: 0.6872684960648164\n",
      "Iteration 8000 finished\n",
      "Iteration 8100 finished\n",
      "Iteration 8200 finished\n",
      "Epoch: 29, Training Loss: 0.6907106052785024, Validation Loss: 0.6872684389857923\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model1.train() \n",
    "    for idx, batch in enumerate(train_iter): \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "\n",
    "        if x.shape[-1] != batch_size:\n",
    "            continue\n",
    "        \n",
    "        it = idx + epoch*len(train_iter)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        preds = model1(x).squeeze()\n",
    "        loss = loss_func(preds, y)\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if it % 100 == 0:\n",
    "            print('Iteration {} finished'.format(it))\n",
    "            \n",
    "    epoch_loss = running_loss / len(train_iter)\n",
    "    epoch_losses1.append(epoch_loss)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model1.eval()\n",
    "    for batch in val_iter:\n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        \n",
    "        if x.shape[-1] != batch_size:\n",
    "            continue\n",
    "        \n",
    "        preds = model1(x).squeeze()\n",
    "        loss = loss_func(preds, y)\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss /= len(val_iter)\n",
    "    val_losses1.append(val_loss)\n",
    "    \n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "accuracy1 = 0\n",
    "true_positive1 = 0\n",
    "false_negative1 = 0\n",
    "false_positive1 = 0\n",
    "\n",
    "model1.eval()\n",
    "\n",
    "for batch in test_iter:\n",
    "    x = batch.text\n",
    "    y = batch.label\n",
    "    \n",
    "    if x.shape[-1] != batch_size:\n",
    "        break\n",
    "    \n",
    "    preds = model1(x)\n",
    "    preds = preds.argmax(dim=-1)\n",
    "    \n",
    "    accuracy1 += (preds == y).sum()\n",
    "    true_positive1 += ((preds == y) * y * preds).sum()\n",
    "    false_positive1 += ((preds != y) * (1 - y) * preds).sum()\n",
    "    false_negative1 += ((preds != y) * y * (1 - preds)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy1 = accuracy1.item()\n",
    "true_positive1 = true_positive1.item()\n",
    "false_positive1 = false_positive1.item()\n",
    "false_negative1 = false_negative1.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy1 = accuracy1 / batch_size / len(test_iter) * 100\n",
    "precision1 = true_positive1 / (true_positive1 + false_positive1)\n",
    "recall1 = true_positive1 / (true_positive1 + false_negative1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 49.9161%\n",
      "Precision 0.4776\n",
      "Recall 0.0077\n",
      "F1 0.0152\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy {}%'.format(round(accuracy1, 4)))\n",
    "print('Precision {}'.format(round(precision1, 4)))\n",
    "print('Recall {}'. format(round(recall1, 4)))\n",
    "print('F1 {}'.format(round(2 * precision1 * recall1 / (precision1 + recall1), 4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
